# -*- coding: utf-8 -*-
"""Stroke Prediction Using Logistic Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rRcxdpgtz4rS6RvLYcBFvuj55QoRDZGp
"""

''' In this project, I will be using machine learning to create a logistic model that will predict whether a person has a stroke
    based on parameters such as gender, age, hypertension, heart disease, BMI, etc.
    I obtained this dataset on Kaggle (https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset)
'''

# We first import the required libraries

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Now, we import our dataset into a Pandas dataframe

df = pd.read_csv('/content/healthcare-dataset-stroke-data.csv')

# We look for general information about our data. We see that we have some object data types which we will deal with. 
df.info()

# We explore the gender column. We first see the value counts. We see that there is only Other entry. Since this 
# single entry does not impact the dataset as a whole, we will drop this row. 

df['gender'].value_counts()

# We first find the index of the row this entry is located and then drop it.

df[df.eq("Other").any(axis=1)]
df = df.drop(index=3116)

# Now we convert the entries of this column into 1 for Male and 0 for Female
df['gender'] = df['gender'].replace(['Male', 'Female'], [1, 0])

# Similarly, since the columns ever_married and Residence_type have only two possible values we convert them into binary values.

df['ever_married'] = df['ever_married'].replace(['Yes', 'No'], [1, 0])
df['Residence_type'] = df['Residence_type'].replace(['Urban', 'Rural'], [1, 0])

# Now, we explore the smoking_status

df['smoking_status'].value_counts()

# Since there are 1544 unknown values we can assume that they correspond to the never smoked category since this 
# is the mode of this column.

df['smoking_status'] = df['smoking_status'].replace(['Unknown'], ['never smoked'])

# Now, we see that the BMI column have some null values, so we fill them with the average. 
df['bmi'].fillna(value=df['bmi'].mean(), inplace=True)
df.reset_index(drop=True)

# Lastly, we see that the columns work_type and smoking_status are categorical features that have more than 2 values.
# Therefore, we implement one hot encoder.

df = df.join(pd.get_dummies(df['work_type'])).drop(['work_type'], axis = 1)
df = df.join(pd.get_dummies(df['smoking_status'])).drop(['smoking_status'], axis = 1)

# Now that we have dealt with our data, we can start doing some further exploration.
# We will create a heatmap to see the correlation between the features.

plt.figure(figsize=(15,8))
sns.heatmap(df.corr(), annot=True, cmap="YlGnBu")

# Based on this heatmap we see that the features age, hypertension, ever_married, and avg_glucose_level have the highest 
# correlation with stroke. The other features have similar correlations so we should not drop any.

# Since age and avg_glucose_level are the numerical features that have the strongest correlation with stroke, 
# we need to check what distribution they follow.

df['age'].hist()

df['avg_glucose_level'].hist()

# We see that while age follows somewhat a normal distribution, avg_glucose_level is skewed. So we normalize it.

df['avg_glucose_level'] = np.log(df['avg_glucose_level'] + 1)
df['avg_glucose_level'].hist()

# Now that we have explored and cleaned our data, let's prepare our data for training. 

# We create our sample matrix X by removing the columns id and stroke and create our target vector y
# by assigning the column stroke to it.

X = df.drop(['id','stroke'], axis=1)
y = df['stroke']

# Now we split our data into training and validation with a 90/10 split.

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)

# Now we are ready to train our model. We use a logistic regression from Scikitlearn. 

log_reg = LogisticRegression(random_state=0, max_iter=1000)

model = log_reg.fit(X_train,y_train)

predictions = model.predict(X_val)
accuracy_score(y_val, predictions)

# Great! we see that our accuracy is about 94.5%.
